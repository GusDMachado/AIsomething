Interesses:
https://www.aicube.ca/
https://cognition.ai/
https://e2b.dev

Git: https://github.com/e2b-dev/E2B
E2B Home: https://e2b.dev/dashboard/gusdmachado/sandboxes
E2B Quick Start: https://e2b.dev/docs/quickstart


webmotors_crawler/
├── .env
├── crawler.py
├── requirements.txt

BASE_URL=https://www.webmotors.com.br/carros/estoque

requests
beautifulsoup4
python-dotenv

--------------

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Carregar variáveis do .env
load_dotenv()
BASE_URL = os.getenv("BASE_URL")

def buscar_carros():
    print(f"Buscando dados em {BASE_URL}")

    # Definir cabeçalhos para simular um navegador
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(BASE_URL, headers=headers)
    if response.status_code != 200:
        print(f"Erro ao acessar a página: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')

    carros = []
    # Ajuste os seletores conforme a estrutura atual do site
    for item in soup.select('.card-listing'):
        modelo = item.select_one('.card-listing__title').text.strip()
        preco = item.select_one('.card-listing__price').text.strip()
        ano = item.select_one('.card-listing__year').text.strip()

        carros.append({
            'modelo': modelo,
            'preco': preco,
            'ano': ano
        })

    return carros

if __name__ == "__main__":
    carros = buscar_carros()
    for carro in carros:
        print(carro)

---------------------

Navegue até o diretório do projeto:
cd webmotors_crawler

Instale as dependências:
pip install -r requirements.txt

Execute o script:
python crawler.py



